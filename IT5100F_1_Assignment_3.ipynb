{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs0H445wCazf"
      },
      "source": [
        "# IT5100F - Assignment 3\n",
        "Semester: AY2024/2025 Semester 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFyN_3nPsK6T"
      },
      "source": [
        "# **Task 1: Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPjVdkhGC6EI"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oHspBTXhC9Aa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import euclidean\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktyc8Dm9ClxC"
      },
      "source": [
        "## 0: Data loading\n",
        "\n",
        "### Load the data from previous assignments\n",
        "\n",
        "For the project, we will continue to use the expanded dataset produced in Assignment 1. Here for the convenience of project development and evaluation, we have two options for data loading, one is loading the dataset from Google Drive(Option 1), another is loading dataset from our server(Option 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KYtKwXqG0BJF"
      },
      "outputs": [],
      "source": [
        "option = 2 # 1 for Google Drive, 2 for url loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GtUAS6A2Cgyg"
      },
      "outputs": [],
      "source": [
        "if option == 1:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  endomondo_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IT5100F/endomondo_proper_cleaned_expanded.csv\")\n",
        "elif option == 2:\n",
        "  url = \"https://nextcloud.wu.engineer/index.php/s/md5dL6XXjjBYBRG/download/endomondo_proper_cleaned_expanded.csv\"\n",
        "  storage_options = {'User-Agent': 'Mozilla/5.0'}\n",
        "  endomondo_df = pd.read_csv(url, storage_options=storage_options)\n",
        "else:\n",
        "  raise ValueError(\"Invalid option. Please choose 1 or 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G6zkpmt13gF"
      },
      "source": [
        "By displaying the first few rows of the dataset, we can observe the types of features it provides and gain a basic understanding of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T0_2bFuFEDRx",
        "outputId": "261f6ec1-403e-4df6-8c4e-6db21df72546"
      },
      "outputs": [],
      "source": [
        "endomondo_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPN5U57l3AXz"
      },
      "source": [
        "The `info()` method prints information about the DataFrame, including the index dtype, columns, non-null values, and memory usage. It also provides a concise summary of the endomondo_df for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDne31quEKm1",
        "outputId": "fd692bda-ebd6-4ca4-9a1e-7da2b10a0877"
      },
      "outputs": [],
      "source": [
        "endomondo_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE1-c2ZYFG5w"
      },
      "source": [
        "## 1: Data Preprocessing for Clustering\n",
        "\n",
        "### 1.1 Filter Data for Specific Sport\n",
        "\n",
        "First, we need to filter the dataset to include only users participating in the sport _Bike_. Before applying the filter, we use the `value_counts()` method to identify the different types of sports and the number of entries for each sport in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb2uS_GOEe8B",
        "outputId": "ef5fdd8d-ad9b-4bb6-abb2-aa2cde62a86b"
      },
      "outputs": [],
      "source": [
        "sports_counts = endomondo_df['sport'].value_counts()\n",
        "print(sports_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUhXFaNu40Ok"
      },
      "source": [
        "Here, the endomondo_df dataset includes various sports such as `bike`, `run`, and `bike (transport)`. To filter out the entries where the sport is `bike`, we use `==` to generate a boolean sequence. This sequence is then used to index the dataframe, returning only the rows where the sport is `bike`. We apply the `copy()` method to create an independent copy of the original dataframe, ensuring any modifications made later won't affect the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DvWO6FheE16o"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset to include only users participating in the sport \"bike\"\n",
        "bike_df = endomondo_df[endomondo_df['sport'] == 'bike'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sg_Isjx7wKm"
      },
      "source": [
        "Before proceeding to the next step, we notice that the data types of the `id` and `timestamp` fields are `float` and `str`, respectively, which are not appropriate. We will convert them to `int` and `datetime` types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0LMWwjn7uC7",
        "outputId": "04b3549a-5703-4c78-aa4d-527c1d8804f9"
      },
      "outputs": [],
      "source": [
        "print(type(bike_df.loc[0,'id']))\n",
        "print(type(bike_df.loc[0,'timestamp']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qOt0Q1-u7iam"
      },
      "outputs": [],
      "source": [
        "# Convert 'timestamp' column to datetime objects\n",
        "bike_df['timestamp'] = pd.to_datetime(bike_df['timestamp'])\n",
        "\n",
        "# Convert 'id' colum to int\n",
        "bike_df['id'] = bike_df['id'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ49zk009ryW"
      },
      "source": [
        "Let's check the type again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leTPYPd99vHI",
        "outputId": "d19b7035-d688-4dac-cd59-0e0b59b94f90"
      },
      "outputs": [],
      "source": [
        "bike_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "68Ie798W9EbQ",
        "outputId": "db5d240b-0fa1-4e0c-dd6b-b424abf7cb6a"
      },
      "outputs": [],
      "source": [
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf_ArJX3ATNR"
      },
      "source": [
        "Although we dropped all the NA values in Assignment 1, we will check for NA values again to ensure no NA values are present in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDmDoZ9KARed",
        "outputId": "b621e03d-cf3e-4ee9-935b-c6155f46edbf"
      },
      "outputs": [],
      "source": [
        "bike_df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfCWUIXz90GK"
      },
      "source": [
        "At this point, we have successfully obtained a subset of the original dataset that only includes users who participated in sports `bike`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmyiCTggFZPA"
      },
      "source": [
        "### 1.2 Generate Average Speed Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utzJBEwsBHwH"
      },
      "source": [
        "Create a new dataset with `user_ids` and their average speed (`avg_speed`).\n",
        "\n",
        "Here we notice the corresponding field of `user_ids` is `id`. Using the `groupby()` method, we group all entries by `id`. Then, we select the `speed` column to return only speed values, and apply the `mean()` method to calculate the average speed. The result is a DataFrame where `id` is the index and the average speed is the value. Finally, we use `reset_index()` to make `id` a separate column and rename the speed column to `avg_speed`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "-Jq0_H6GFCFY",
        "outputId": "0f9f4202-80c1-4b4d-a324-6c1f0fc47de5"
      },
      "outputs": [],
      "source": [
        "# Create a new dataset with user_ids and their average speed (avg_speed)\n",
        "\n",
        "avg_speed_df = bike_df.groupby('id')['speed'].mean().reset_index()\n",
        "avg_speed_df = avg_speed_df.rename(columns={'speed': 'avg_speed'})\n",
        "print(len(avg_speed_df))\n",
        "avg_speed_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9jbZub0GChI"
      },
      "source": [
        "### 1.3 Compute Total Workout Time\n",
        "\n",
        "Create a new dataset with `user_ids` and their total workout time (`workout_time`), calculated as the difference between the minimum and maximum timestamp in seconds for each user.\n",
        "\n",
        "First, we group the DataFrame by `id` and aggregate the timestamp values using the `agg()` function to compute both the minimum and maximum timestamps for each user. We use `reset_index()` to flatten the grouped data into a DataFrame with a new index.\n",
        "\n",
        "Next, we calculate the workout duration by finding the difference between the maximum and minimum timestamps for each user and convert this difference into seconds using `dt.total_seconds()`. We store this result in a new column called `workout_time`.\n",
        "\n",
        "Finally, we retain only the `id` and `workout_time` columns showing the total workout time for each user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r8OMaAPSGD02",
        "outputId": "dbf98cf7-1f09-49b7-aef4-e8065c57e328"
      },
      "outputs": [],
      "source": [
        "# Create a new dataset with user_ids and their total workout time (workout_time),\n",
        "# calculated as the difference between the minimum and maximum timestamp in seconds for each user.\n",
        "\n",
        "\n",
        "# Group by user ID and calculate the total workout time\n",
        "workout_time_df = bike_df.groupby('id')['timestamp'].agg(['min', 'max']).reset_index()\n",
        "workout_time_df['workout_time'] = (workout_time_df['max'] - workout_time_df['min']).dt.total_seconds()\n",
        "workout_time_df = workout_time_df[['id', 'workout_time']]\n",
        "\n",
        "workout_time_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z13nwga5GyZg"
      },
      "source": [
        "### 1.4 Compute Total Distance Covered\n",
        "\n",
        "Create a new dataset with user_ids and the total distance (total_distance) covered by each user.\n",
        "\n",
        "To obtain total distance for each user, we use $\\text{distance}=\\text{avg_speed} \\times \\text{workout_time}$ formula. So first, we merge the previous two DataFrame, then multiply `avg_speed` column and `workout_time` column to get `total_distance`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5RgUyAQjG1gm",
        "outputId": "c17377b0-5436-4628-ba8c-f266062ff94c"
      },
      "outputs": [],
      "source": [
        "# Create a new dataset with user_ids and the total distance (total_distance) covered by each user.\n",
        "total_distance_df = pd.merge(workout_time_df, avg_speed_df, on='id')\n",
        "total_distance_df['total_distance'] = total_distance_df['avg_speed'] * total_distance_df['workout_time']\n",
        "total_distance_df = total_distance_df[['id', 'total_distance']]\n",
        "total_distance_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSR2GYTDfaEq"
      },
      "source": [
        "### 1.5 Merge Processed Data\n",
        "\n",
        "Merge all the above datasets into a single data frame called `user_merged_df`.\n",
        "To merge all the DataFrame together, We set `id` as index, then use `pd.concat` function to concatenate three DataFrame together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tsHtAjtvetg3",
        "outputId": "ba14e783-deb5-48ef-da1e-25be8232cab1"
      },
      "outputs": [],
      "source": [
        "# Merge all the above datasets into a single data frame called user_merged_df.\n",
        "\n",
        "avg_speed_df.set_index(\"id\", inplace=True)\n",
        "total_distance_df.set_index(\"id\", inplace=True)\n",
        "workout_time_df.set_index(\"id\", inplace=True)\n",
        "\n",
        "user_merged_df = pd.concat(\n",
        "    [avg_speed_df, workout_time_df, total_distance_df], axis=1\n",
        ").reset_index()\n",
        "\n",
        "user_merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0OdY5xcjU1v"
      },
      "source": [
        "## 2: Determine the Optimal Number of Clusters\n",
        "\n",
        "### 2.1 Run K-Means for Different Cluster Numbers\n",
        "\n",
        "Use the user_merged_df to run k-means clustering from 2-11 clusters.\n",
        "\n",
        "To perform k-means clustering, the `id` field should not be used as a feature, so we drop it first. Then, we use `StandardScaler` to remove the mean and scale the data to unit variance. Since k-means uses Euclidean distance, features with larger ranges would dominate the distance calculation if we don't apply standard scaling.\n",
        "\n",
        "Next, we run clustering with 2 to 11 clusters and collect the inertia values. We set `random_state` to ensure reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIzD6xalfljd",
        "outputId": "0983f568-d44f-432c-9311-8e5bc9817a2b"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = user_merged_df.drop(columns=[\"id\"])\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "inertia = []\n",
        "cluster_range = range(2, 12)\n",
        "\n",
        "for n_clusters in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "print(inertia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8DN-lxloVr"
      },
      "source": [
        "### 2.2 Elbow Method for Optimal Clusters\n",
        "\n",
        "Plot the inertia (y-axis) against the number of clusters (x-axis) to identify the optimal number of clusters using the Elbow method.\n",
        "\n",
        "The Elbow Method helps in selecting the number of clusters by identifying where the inertia starts to decrease more slowly.\n",
        "\n",
        "From our graph, inertia starts to decrease more slowly when the cluster number comes to 6. So we think the best cluster number is 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "nT7BXoaelr2f",
        "outputId": "1460d733-5e4c-46f0-e161-7b070a902222"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(cluster_range, inertia, marker='o', linestyle='-', color='b')\n",
        "plt.title('K-Means Clustering Inertia for 2 to 11 Clusters')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xticks(cluster_range)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kkBgZBphmOO8"
      },
      "outputs": [],
      "source": [
        "best_cluster_number = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRWmo9y3l7zG"
      },
      "source": [
        "## 3: Cluster Analysis and Visualization\n",
        "\n",
        "### 3.1 Identify the Cluster Number for Each User\n",
        "\n",
        "Create a k-means model using the features from `user_merged_df` and the optimal number of clusters identified in Task 2.\n",
        "\n",
        "Add a new column to the data frame called `cluster` that contains the cluster number for each user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6s84l_aPmG4e",
        "outputId": "4a5994ca-63fa-4121-b1a9-b2fedfebd937"
      },
      "outputs": [],
      "source": [
        "# Create a k-means model using the features from user_merged_df and the optimal number of clusters identified in Task 2.\n",
        "\n",
        "kmeans = KMeans(n_clusters=best_cluster_number, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "\n",
        "# Add a new column to the data frame called cluster that contains the cluster number for each user.\n",
        "user_merged_df['cluster'] = kmeans.labels_\n",
        "user_merged_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWXLBGvimyEf"
      },
      "source": [
        "### 3.2 Visualize the Clusters\n",
        "\n",
        "Visualize the clusters using a scatter plot.\n",
        "\n",
        "Because there are three features in the dataset, we choose 3D scatter plot to display the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "cPkyYyTuQvDo",
        "outputId": "7d1de330-4582-433e-addc-cbd0aec4c999"
      },
      "outputs": [],
      "source": [
        "# Visualize the clusters using a scatter plot. (3d)\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(user_merged_df['avg_speed'],\n",
        "                     user_merged_df['total_distance'],\n",
        "                     user_merged_df['workout_time'],\n",
        "                     c=user_merged_df['cluster'],\n",
        "                     cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('Average Speed')\n",
        "ax.set_ylabel('Total Distance')\n",
        "ax.set_zlabel('Workout Time')\n",
        "ax.set_title('3D Visualization of Clusters')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dNmXHdTSlWj"
      },
      "source": [
        "For better visualization, we first apply Principal Component Analysis to reduce the dimensionality of the features from 3 to 2. This allows us to project the data into two principal components, which can then be visualized using a 2D scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "Hp4oS2kkTCQL",
        "outputId": "b7e6e3fe-8715-4272-f0cc-60154e7ce351"
      },
      "outputs": [],
      "source": [
        "# Use PCA to reduce dimension from 3 to 2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "pca_df['cluster'] = user_merged_df['cluster'].values\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='cluster', palette='viridis', data=pca_df)\n",
        "plt.title('PCA - 2D Visualization with Cluster Colors', fontsize=14)\n",
        "plt.xlabel('Principal Component 1', fontsize=12)\n",
        "plt.ylabel('Principal Component 2', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2zlgOGHT1-C"
      },
      "source": [
        "Then we try t-SNE method for dimensionality reduction. In our practice, we reduce the features to 2 components for visualization using a 2D scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "VSIRXMw9rMSf",
        "outputId": "dcbba7df-586e-441e-e518-e11ae56e5763"
      },
      "outputs": [],
      "source": [
        "# Use t-SNE for dimension reduction\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_result = tsne.fit_transform(X)\n",
        "user_merged_df['tsne1'] = tsne_result[:, 0]\n",
        "user_merged_df['tsne2'] = tsne_result[:, 1]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x='tsne1', y='tsne2', hue='cluster', data=user_merged_df, palette='viridis')\n",
        "plt.title('t-SNE of Clusters')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAfA8ODufXE9"
      },
      "source": [
        "### 3.3 Identify Similar Users\n",
        "\n",
        "Determine which cluster the user ID: 377398220 belongs to.\n",
        "\n",
        "Identify similar users (workout buddies) within that cluster so that you can make recommendations based on the shared workout patterns.\n",
        "\n",
        "To find which cluster a user belongs to, we use the == operation on the id field.\n",
        "\n",
        "After identifying the user's cluster, we retrieve users from the same cluster (except the target user with ID 377398220). Next, we compute the similarity score using the Euclidean distance between the target user and each user in the same cluster. We calculate the Euclidean distance based on relevant features (`avg_speed` and `total_distance`) and then sort users by this distance. This allows us to identify the top-K users who are most similar to the target user based on the Euclidean distance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkntjNiDfYaO",
        "outputId": "fb5d227e-2ad0-4b65-d5a5-e8c229cae384"
      },
      "outputs": [],
      "source": [
        "target_user_id = 377398220\n",
        "target_user = user_merged_df[user_merged_df['id'] == target_user_id].iloc[0]\n",
        "target_user_cluster = target_user['cluster']\n",
        "\n",
        "similar_users = user_merged_df[\n",
        "    (user_merged_df['cluster'] == target_user_cluster) &\n",
        "    (user_merged_df['id'] != target_user_id)\n",
        "].copy()\n",
        "\n",
        "target_user_features = target_user[['avg_speed', 'total_distance']].values\n",
        "\n",
        "similar_users['similarity'] = similar_users.apply(\n",
        "    lambda row: euclidean(row[['avg_speed', 'total_distance']], target_user_features),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "similar_users = similar_users.sort_values('similarity')\n",
        "\n",
        "print(f\"Target User (ID: {target_user_id}):\")\n",
        "print(f\"Cluster: {target_user_cluster}\")\n",
        "print(f\"Average Speed: {target_user['avg_speed']:.2f} km/h\")\n",
        "print(f\"Total Distance: {target_user['total_distance']:.2f} km\")\n",
        "print(f\"Workout Time: {target_user['workout_time']:.2f} seconds\")\n",
        "print(\"\\nTop 5 similar users:\")\n",
        "for _, user in similar_users.head().iterrows():\n",
        "    print(f\"User ID: {user['id']}\")\n",
        "    print(f\"  Average Speed: {user['avg_speed']:.2f} km/h\")\n",
        "    print(f\"  Total Distance: {user['total_distance']:.2f} km\")\n",
        "    print(f\"  Workout Time: {user['workout_time']:.2f} seconds\")\n",
        "    print(f\"  Similarity Score: {user['similarity']:.2f}\")\n",
        "    print()\n",
        "\n",
        "cluster_avg = user_merged_df[user_merged_df['cluster'] == target_user_cluster].mean()\n",
        "print(f\"Cluster {target_user_cluster} Averages:\")\n",
        "print(f\"Average Speed: {cluster_avg['avg_speed']:.2f} km/h\")\n",
        "print(f\"Total Distance: {cluster_avg['total_distance']:.2f} km\")\n",
        "print(f\"Workout Time: {cluster_avg['workout_time']:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkbo00f0icTX"
      },
      "source": [
        "# **Task 2: Free-form Exploration**: Sport Classfication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmcfGIxmjlCn"
      },
      "source": [
        "# Problem definition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWgqo1akijQG"
      },
      "source": [
        "The objective of this task is to develop a machine learning model capable of classifying different types of sports activities based on aggregated user data from the Endomondo fitness tracking application. This classification problem presents several interesting challenges and considerations:\n",
        "\n",
        "1. **Dataset**:\n",
        "   - We will be using the `endomondo_proper_cleaned_expanded.csv` dataset, which contains detailed tracking information for various sports activities.\n",
        "\n",
        "2. **Feature Selection**:\n",
        "   Our classification will be based on the following key factors:\n",
        "   - Average altitude of each user\n",
        "   - Average heart rate of each user\n",
        "   - Average latitude and longitude of each user\n",
        "   - Average speed of each user\n",
        "   - Total time spent doing sports for each user\n",
        "\n",
        "3. **Data Preprocessing**:\n",
        "   - We will aggregate the data by calculating the average of each factor per user. This approach serves two purposes:\n",
        "     * It reduces the size of the dataset, making it more manageable for analysis.\n",
        "     * It provides a holistic view of each user's activity patterns, which may be more indicative of the type of sport they engage in most frequently.\n",
        "\n",
        "4. **Class Imbalance**:\n",
        "   - The dataset likely exhibits imbalance among different sports classes, with some activities being more common than others. This imbalance needs to be addressed to ensure fair classification across all sports.\n",
        "\n",
        "5. **Geospatial Considerations**:\n",
        "   - The inclusion of average latitude and longitude introduces a geospatial element to our classification. We need to consider how geographical location might influence sport classification.\n",
        "\n",
        "6. **Temporal Aspects**:\n",
        "   - The total time spent on sports activities provides a temporal dimension to our data. We should consider how this might interact with other features (e.g., average speed) to distinguish between different sports.\n",
        "\n",
        "7. **Feature Scaling**:\n",
        "   - Given the diverse nature of our features (altitude, heart rate, geographical coordinates, speed, and time), appropriate scaling or normalization will be crucial to ensure all features contribute fairly to the classification.\n",
        "\n",
        "8. **Model Selection and Evaluation**:\n",
        "   - We need to select an appropriate machine learning algorithm that can handle multi-class classification with potentially imbalanced data.\n",
        "   - Evaluation metrics should go beyond simple accuracy, considering precision, recall, and F1-score for each sport category.\n",
        "\n",
        "This model could have practical applications in personalizing fitness recommendations, improving activity recognition in tracking apps, and gaining insights into how different sports are characterized by various physiological and environmental factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ecOyYy8U69"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NdixMrC9LVa"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-2r1Q2cj9LVb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIADu79v9LVb"
      },
      "source": [
        "## 0: Data loading\n",
        "\n",
        "### Load the data from previous assignments\n",
        "\n",
        "For the project, we will continue to use the expanded dataset produced in Assignment 1. Here for the convenience of project development and evaluation, we have two options for data loading, one is loading the dataset from Google Drive(Option 1), another is loading dataset from our server(Option 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vRox0KfI9LVb"
      },
      "outputs": [],
      "source": [
        "option = 2 # 1 for Google Drive, 2 for url loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "iLSPldoYZSFH"
      },
      "outputs": [],
      "source": [
        "if option == 1:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  endomondo_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/IT5100F/endomondo_proper_cleaned_expanded.csv\")\n",
        "elif option == 2:\n",
        "  url = \"https://nextcloud.wu.engineer/index.php/s/md5dL6XXjjBYBRG/download/endomondo_proper_cleaned_expanded.csv\"\n",
        "  storage_options = {'User-Agent': 'Mozilla/5.0'}\n",
        "  endomondo_df = pd.read_csv(url, storage_options=storage_options)\n",
        "else:\n",
        "  raise ValueError(\"Invalid option. Please choose 1 or 2.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WBfEyu609LVc",
        "outputId": "6c8bb4e7-2ee4-4f4a-e8b3-d6b76b0475ff"
      },
      "outputs": [],
      "source": [
        "endomondo_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8pTjfRK9LVc",
        "outputId": "50fc2203-d3e6-42bb-d7d1-d3fc2665e6af"
      },
      "outputs": [],
      "source": [
        "endomondo_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ISr4awFRx-"
      },
      "source": [
        "### 1.1 Calculate Useful Factors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cnUF0TE8FgJ2",
        "outputId": "95cc81b8-8e9e-4d93-fc70-c6abec649b3e"
      },
      "outputs": [],
      "source": [
        "# Convert 'id' column to int\n",
        "endomondo_df['id'] = endomondo_df['id'].astype(\"Int64\")\n",
        "\n",
        "# Calculate the average value of useful columns\n",
        "avg_df = endomondo_df.groupby(['id','sport'])[['altitude', 'heart_rate', 'latitude', 'longitude', 'speed']].mean()\n",
        "avg_df = avg_df.rename(columns={'altitude': 'avg_altitude','heart_rate': 'avg_heart_rate','latitude': 'avg_latitude','longitude': 'avg_longitude','speed': 'avg_speed'})\n",
        "\n",
        "# Convert 'timestamp' column to datetime objects\n",
        "endomondo_df['timestamp'] = pd.to_datetime(endomondo_df['timestamp'])\n",
        "\n",
        "# Calculate the total time duration\n",
        "total_time_df = endomondo_df.groupby(['id','sport'])['timestamp'].agg(['min', 'max'])\n",
        "total_time_df['total_time'] = (total_time_df['max'] - total_time_df['min']).dt.total_seconds()\n",
        "\n",
        "# Merge the two dataframes\n",
        "new_endomondo_df = pd.merge(avg_df, total_time_df, on=['id', 'sport']).reset_index()\n",
        "\n",
        "# Drop the useless 'min' and 'max' columns\n",
        "new_endomondo_df = new_endomondo_df.drop(columns=['min', 'max'])\n",
        "\n",
        "new_endomondo_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7a2Bw66sHkG"
      },
      "source": [
        "### 1.2 Explore class distributions after aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "3wwCIWunsGbw",
        "outputId": "b94d5536-712c-4a5b-9ad8-cc496a624b1b"
      },
      "outputs": [],
      "source": [
        "class_distribution = new_endomondo_df['sport'].value_counts()\n",
        "print(\"Class Distribution:\")\n",
        "print(class_distribution)\n",
        "plt.figure(figsize=(12, 6))\n",
        "class_distribution.plot(kind='bar')\n",
        "plt.title('Distribution of Sports Classes')\n",
        "plt.xlabel('Sport')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFGguXfSwLF-"
      },
      "source": [
        "From the distribution above, we can observe that many sport classes have very few data samples of unique users after aggregation. These data cannot provide enough information for classfication, instead, these may introduce noise to our dataset. Hence, we decide to remove those sports with samples less than 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjuj9chKrqhy"
      },
      "source": [
        "### 1.3 Drop sports with very few users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dt_2eCyVrvt5"
      },
      "outputs": [],
      "source": [
        "# Count the number of samples for each sport\n",
        "sport_counts = new_endomondo_df['sport'].value_counts()\n",
        "\n",
        "# Keep only the sports with at least 100 user samples\n",
        "sports_to_keep = sport_counts[sport_counts >= 100].index\n",
        "\n",
        "# Filter the dataframe to keep only the selected sports\n",
        "filtered_df = new_endomondo_df[new_endomondo_df['sport'].isin(sports_to_keep)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "p-M0qqNLsbIS",
        "outputId": "1c0ce916-7333-4b68-dce8-1fe14cff37ea"
      },
      "outputs": [],
      "source": [
        "class_distribution = filtered_df['sport'].value_counts()\n",
        "print(\"Class Distribution:\")\n",
        "print(class_distribution)\n",
        "plt.figure(figsize=(12, 6))\n",
        "class_distribution.plot(kind='bar')\n",
        "plt.title('Distribution of Sports Classes')\n",
        "plt.xlabel('Sport')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpGfj0WJw3eg"
      },
      "source": [
        "After filtering, we are actually classifying five sports:\n",
        "\n",
        "1.  bike\n",
        "2.  run\n",
        "3.  bike(transport)\n",
        "4. mountain bike\n",
        "5. indoor cycling.\n",
        "\n",
        "using the aggregation features we got from each user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym1YIkTdMakX"
      },
      "source": [
        "## 2: Model Training and Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UcTUHGHNE2w"
      },
      "source": [
        "### 2.1 Define the Features and Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TGyB9m1wMY2K"
      },
      "outputs": [],
      "source": [
        "# Features to use as input for the model\n",
        "X_Features = ['avg_altitude', 'avg_heart_rate','avg_latitude','avg_longitude','avg_speed','total_time']\n",
        "\n",
        "# Features to predict\n",
        "Y_Features = ['sport']\n",
        "\n",
        "X = filtered_df[X_Features]\n",
        "Y = filtered_df[Y_Features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snnz7uqZQIzX"
      },
      "source": [
        "### 2.2 Split the data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Hz6i9WD7Px48"
      },
      "outputs": [],
      "source": [
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Split the data into training and testing data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnV8KQcgTExe"
      },
      "source": [
        "### 2.3 Build the Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "Y4YcuVooQQvd",
        "outputId": "68387592-5eb8-46ff-d1f1-f70bc3c155e9"
      },
      "outputs": [],
      "source": [
        "rf_N=100\n",
        "\n",
        "# Initialization Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=rf_N, random_state=RANDOM_STATE)\n",
        "\n",
        "# Training Model\n",
        "rf.fit(X_train_scaled, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTzX6rVRTO_C"
      },
      "source": [
        "### 2.4 Calculate the prediction accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "ilQghkMDTPXD",
        "outputId": "324b4099-c0f7-49f1-9084-d4dc25fb34fa"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "Y_pred = rf.predict(X_test_scaled)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "# Normalize Confusion Matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, None]\n",
        "\n",
        "# Draw Normalized Confusion Matrix through seaborn\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues')\n",
        "plt.xlabel('Predicted Sports')\n",
        "plt.ylabel('Actual Sports')\n",
        "plt.title('Normalized Confusion Matrix of Sports')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au4JKumYxmMA"
      },
      "source": [
        "Based on the classification report, the model performs well overall with an accuracy of 0.95 and a weighted average F1-score of 0.95. However, the macro average F1-score (0.80) is lower than the weighted average (0.95), indicating that the model's performance varies significantly across classes, with poorer performance on minority classes. Performance for \"bike (transport)\", \"indoor cycling\", and \"mountain bike\" is notably lower, with F1-scores ranging from 0.54 to 0.76.\n",
        "\n",
        "To address the class imbalance issue and potentially improve the model's performance on minority classes, we plan to use SMOTE (Synthetic Minority Over-sampling Technique) to oversample the minority classes. This approach will create synthetic examples of the underrepresented classes, balancing the dataset.\n",
        "\n",
        "By applying SMOTE and retraining the model, we aim to:\n",
        "\n",
        "* Improve the F1-scores for the minority classes (bike transport, indoor cycling, mountain bike).\n",
        "* Achieve a more balanced performance across all classes, potentially increasing the macro average F1-score.\n",
        "* Maintain or possibly improve the overall accuracy while ensuring better representation of all sports categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaO0xFowy9dM"
      },
      "source": [
        "### 3.1 Try using SMOTE and retrain the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "kXatK5j3rB9W",
        "outputId": "f29cd67d-d68e-4a52-e10e-3dc65186e0d6"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, Y_train)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "Y_pred = rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "# Normalize Confusion Matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, None]\n",
        "\n",
        "# Draw Normalized Confusion Matrix through seaborn\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues')\n",
        "plt.xlabel('Predicted Sports')\n",
        "plt.ylabel('Actual Sports')\n",
        "plt.title('Normalized Confusion Matrix of Sports')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFZoztp40FJK"
      },
      "source": [
        "The use of SMOTE has indeed improved the classification results, particularly for the minority classes. Let's break down the improvements and discuss real-life applications:\n",
        "\n",
        "Improvements after SMOTE:\n",
        "\n",
        "1. Minority classes performance:\n",
        "   - Bike (transport): F1-score improved from 0.73 to 0.79\n",
        "   - Indoor cycling: F1-score slightly decreased from 0.76 to 0.72, but recall improved from 0.68 to 0.75\n",
        "   - Mountain bike: F1-score significantly improved from 0.54 to 0.65\n",
        "\n",
        "2. Balanced performance:\n",
        "   - Macro avg F1-score increased from 0.80 to 0.82, indicating better overall performance across all classes\n",
        "   - Recall for minority classes also improved, showing better detection of these activities\n",
        "\n",
        "3. Maintained overall accuracy:\n",
        "   - The overall accuracy remained at 0.95, while improving minority class performance\n",
        "\n",
        "Real-life applications of this classfication model:\n",
        "\n",
        "1. Fitness tracking apps: The model can automatically classify different cycling activities, providing users with more accurate activity logs and tailored feedback.\n",
        "\n",
        "2. Urban planning: By accurately distinguishing between regular cycling and transport cycling, city planners can better understand commuting patterns and design appropriate infrastructure.\n",
        "\n",
        "\n",
        "\n",
        "This improved model shows the value of addressing class imbalance in real-world datasets, leading to more reliable and equitable performance across different activities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
